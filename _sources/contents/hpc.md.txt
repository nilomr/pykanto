

# High Performance Computing 

## Introduction

Many of the processes that `pykanto` carries out are computationally intensive,
such as calculating spectrograms, performing operations on large arrays,
dimensionality reduction and clustering. High-level, interpreted languages—like
R or Python—are notoriously slow: where possible, I have optimised performance
by both a) translating functions to optimized machine code at runtime using
[Numba](https://numba.pydata.org/) and b) parallelising tasks using
[Ray](https://www.ray.io/), a platform for distributed
computing. As an example, the \texttt{segment\_into\_units()} function can find
and segment 20.000 discrete acoustic units in approximately 16 seconds on a
desktop 8-core machine; a dataset with over half a million (556.472) units takes
~132 seconds on a standard 48-core compute node.

`pykanto` works in any (well, hopefully) personal machine, but for most real
applications you will probably want to run it on a compute cluster. This can be a
daunting task, so I have packaged some tools that should make it a little bit
easier.


[Slurm](https://slurm.schedmd.com/documentation.html) is still the most popular
job scheduler used in compute clusters, so the following instructions and tips
refer to it.


````{admonition} Tip: testing your code on your local machine
:class: tip, dropdown

Before you run a large job on a compute cluster you might want to test your
parameters (e.g., for spectrogramming and segmentation) on a local machine. To
do this, you can test-build a `pykanto` dataset from a subset of your data -
large enough to be representative, but small enough to run quickly on your
machine. There are two ways to do this in `pykanto`:

To use a random subset,

```{code-block} python
:linenos:

dataset = SongDataset(... , random_subset=200)
```
To use a slice of the data:

```{code-block} python
:linenos:

params = Parameters(... , subset=(100, 300))
dataset = SongDataset(... , parameters=params)
```
````


The following is adapted from [code by Peng
Zhenghao](https://github.com/pengzhenghao/use-ray-with-slurm).

```{admonition} sbatch script procedure:

1. Fetches the list of computing nodes and their IP addresses. 

2. Launches a head ray process in one of the nodes, and gets the address of the
   head node. 

3. Launches ray processes in (n-1) worker nodes and connects them to the head
node by providing the head node address. 

4. Submits the user-specified task to ray.

Source: [use-ray-with-slurm](https://github.com/pengzhenghao/use-ray-with-slurm)
```



Add this to the top of your script, right after any imports:


```{code-block} python
:linenos:

redis_password = sys.argv[1]
ray.init(address=os.environ["ip_head"], _redis_password=redis_password)
print(ray.cluster_resources())

```

## Miscellaneous

If you need to upload your raw or segmented data to use in a HPC cluster and you
have lots of small files you may want to consider creating a `.tar` file to
reduce overhead. `pykanto` has a simple wrapper function to do this:

```{code-block} python
:linenos:

from pykanto.utils.write import make_tarfile 
out_dir = DIRS.SEGMENTED / 'JSON.tar.gz'
in_dir = DIRS.SEGMENTED / 'JSON'
make_tarfile(out_dir, in_dir)
```

